- `cmd/daisi-cdc-consumer-service/main.go`: Implement main application logic, wire dependencies, and start the service.
- `internal/domain/model.go`: Review and refine domain models if necessary; add validation logic if applicable. // Mostly done; typed structs AgentData, ChatData, MessageData added.
- `internal/domain/ports.go`: Review and refine port interfaces if necessary; consider adding methods or adjusting signatures based on evolving adapter needs. // Updated with IncUnhandledFieldsTotal for MetricsSink.
- `internal/application/consumer.go`: Implement detailed message processing logic: parsing `msg.GetData()` into `domain.CDCEventData`, transformation based on `domain.AllowedTables`, robust `event_id` generation (e.g., LSN:Table:PK), `domain.EnrichedEventPayload` creation, and interaction with `WorkerPool`. Ensure all calls to `configProvider.Get...` use defined constants from the `config` package. // Core logic with typed structs implemented. Remaining: Add comprehensive unit tests for `processEvent` (covering various scenarios like duplicate events, errors, different tables).
- `internal/application/worker_pool.go`: Review and test refined `WorkerPool` sizing logic (override, multiplier, min); ensure proper `ants.Pool` configuration (expiry, panic handling), and robust task submission/management. // Reviewed. Sizing logic implemented. Remaining: Add unit tests for `NewWorkerPool` sizing logic.
- `internal/adapters/nats/ingest.go`: Refine NATS JetStream consumer logic: ensure `processJetStreamMessage` correctly propagates or creates a new context for `HandleCDCEvent` (with `event_id`, `table_name`). Review message ACKs/NACKs, and graceful shutdown.
- `internal/adapters/nats/publish.go`: Implement NATS JetStream publisher logic: connect to NATS, implement `domain.Publisher.Publish` to send messages to the `wa_stream`.
- `internal/adapters/redis/dedup.go`: Implement Redis `SETNX` logic for `domain.DedupStore.IsDuplicate`, connect to Redis, handle Redis client lifecycle.
- `internal/adapters/metrics/prometheus.go`: Implement Prometheus metrics: define collectors for all metrics in `domain.MetricsSink`, register them, expose `/metrics` endpoint using `promhttp`, and start/stop HTTP server. // Adapter implemented, including unhandled fields metric.
- `internal/adapters/config/viper.go`: Review Viper implementation; add tests for loading from ENV, YAML, and defaults; consider hot-reloading if required by later tasks.
- `internal/adapters/logger/zap.go`: Review Zap logger implementation; add tests for log levels, output format, `event_id`/`table_name` injection from context, and `With` functionality.
- `TM Task 4: Implement and Expose Prometheus Metrics`:
  - `Subtask 4.1 (internal/adapters/metrics/prometheus.go)`: Implement MetricsSink adapter. // Done.
  - `Subtask 4.2 (Expose & Integrate)`: Expose /metrics endpoint and integrate MetricsSink. // Pending bootstrap/DI setup.
- `internal/bootstrap/container.go`: Implement dependency injection setup (e.g., using Uber Fx or Google Wire); initialize and start metrics server (using `metrics.StartMetricsServer` with port from `ConfigProvider`), inject `MetricsSink` into `Consumer`, and wire other dependencies (Logger, Config, NATS, Redis once available).
- `cmd/daisi-cdc-consumer-service/main.go`: Implement top-level `defer recover()` in `main` function to catch unhandled panics (especially from the Panic Guard feature of Task 8), log the panic information, and call `os.Exit(1)` to ensure the pod is restarted by the orchestrator.
- **Testing (Panic Guard - Task 8)**: Conduct integration tests to simulate prolonged downstream unavailability (e.g., mock Redis/NATS publisher to always return errors). Verify that the service logs the fatal error via panic recovery and exits after the 15-minute cumulative failure threshold is breached. Also test that successful processing correctly resets the failure counter.
